---
title: "Chapter 3: Linear Methods for Regression"
author: "Bodo Burger"
date: 2018-05
output:
  rmarkdown::github_document:
    toc: true
---

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      cache = TRUE, cache.path = "cache/chapter03/",
                      fig.path = "figures/")
set.seed(123)
library("mlr")
library("ggplot2")
library("leaps")
theme_set(theme_light())
```

# Linear Regression Models


## Figure 3-3 tail probabilities

```{r figure-03-03-tail-probabilities, fig.asp=.5}
z = seq(1.9, 3, .01)
plot(z, 2 * (1 - pt(z, df = 30)), col = "orange", type = "l", lwd = 1, xlab = "Z", ylab = "Tail Probabilities")
lines(z, 2 * (1 - pt(z, df = 100)), col = "deepskyblue")
lines(z, 2 * (1 - pnorm(z)), col = "aquamarine3")
abline(h = c(.01, .05), lty = 2, lwd = .5)
abline(v = qt(c(.975, .995), df = 30), lty = 2, lwd = .5, col = "orange")
abline(v = qt(c(.975, .995), df = 100), lty = 2, lwd = .5, col = "deepskyblue")
abline(v = qnorm(c(.975, .995)), lty = 2, lwd = .5, col = "aquamarine3")
```

## Table 3-1

```{r}
prostate.data = ElemStatLearn::prostate
prostate.cor = round(cor(subset(prostate.data, subset = train, select = 1:8)), digits = 3)
prostate.cor[upper.tri(prostate.cor, diag = "TRUE")] = ""
knitr::kable(prostate.cor[-1, -8])
```

## Table 3-2

```{r}
#prostate.data$svi = factor(prostate.data$svi) 
#prostate.data$gleason = factor(prostate.data$gleason, ordered = TRUE)
#numerical.features = c("lcavol", "lweight", "age", "lbph", "lcp", "pgg45")
prostate.data[-c(9, 10)] = scale(prostate.data[-c(9, 10)])
train.data = subset(prostate.data, subset = train, select = 1:9)
test.data = subset(prostate.data, subset = !train, select = 1:9)
lm.model = lm(lpsa ~ ., data = train.data)
knitr::kable(summary(lm.model)$coefficients[, -4], digits = 2)
```

To reproduce exactly the results from the book we need to standardize all predictor variables. Note that we also standardize **svi** (a factor / dummy variable) and **gleason** (a ordered categorical variable) which seems odd but is suggested for the regularization method that is used below (see [Tibshirani (1997) The LASSO method](http://statweb.stanford.edu/~tibs/lasso/fulltext.pdf)).

# Subset Selection

```{r fitting subset models}
leaps.model = regsubsets(lpsa ~ ., data = train.data, nbest = 70, really.big = TRUE)
prostate.models = summary(leaps.model)$which
prostate.models.size = as.numeric(attr(prostate.models, "dimnames")[[1]])
prostate.models.rss = summary(leaps.model)$rss
prostate.models.best.rss = tapply(prostate.models.rss, prostate.models.size, min)
prostate.intercept.model = lm(lpsa ~ 1, data = train.data)
prostate.models.best.rss = c(sum(residuals(prostate.intercept.model)^2), prostate.models.best.rss)
```

## Figure 3-5 all subset models for prostate cancer example

```{r figure-03-05-subset-models, fig.asp=.7}
ggplot(mapping = aes(x = 0:8, y = prostate.models.best.rss)) +
  geom_point(mapping = aes(x = prostate.models.size, y = prostate.models.rss), col = "slategray") + 
  geom_point(col = "red", size = 2) + geom_line(col = "red") +
  coord_cartesian(ylim = c(0, 100)) + xlab("Subset Size k") + ylab("Residual Sum-of-Squares")
```

The data generating process for Figure 3.6 is described in its subtitle in the book. The estimates are are averaged over 50 simulations

```{r figure-03-06-data, eval=FALSE}
n = 300
p = 31
generateData = function(n, p) {
  # features X:
  mu = rep(0, p)
  sigma = matrix(.85, ncol = p, nrow = p) + diag(.15, p)
  X = mvtnorm::rmvnorm(n, mean = mu, sigma = sigma)
  # coefficients b:
  b = numeric(p)
  b[sample(p, 10)] = rnorm(10, 0, .4)
  # noise eps:
  eps = rnorm(1, 0, 6.25)
  # target y:
  y = X %*% b + eps
  # data.frame
  df = data.frame(y, X)
  return(list(data = df, y = y, X = X, b = b, eps = eps))
}
dgp = generateData(300, 31)
y = dgp$X %*% dgp$b
hist(y)
hist(dgp$y)
dgp$eps

bestsub.model = regsubsets(y ~ ., data = dgp$data, really.big = TRUE)
forstep
backstep
forstage.model


```


# Links
